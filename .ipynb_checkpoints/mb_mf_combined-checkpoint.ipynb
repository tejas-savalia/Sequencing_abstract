{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "environment = numpy.zeros((2, 7))\n",
    "trans_prob = 0.7\n",
    "environment[1, 6] = 10\n",
    "#environment[8, 8] = -10\n",
    "environment[1, 0] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_legal_actions(pos):\n",
    "    legal_actions = list()\n",
    "    new_pos = list()\n",
    "    all_actions = [[1, 0], [0, 1], [-1, 0], [0, -1]]\n",
    "    for i in all_actions:\n",
    "        try:\n",
    "            new_pos = numpy.add(pos, numpy.array(i))\n",
    "            legal_actions.append(i)\n",
    "            env = environment[new_pos[0]][new_pos[1]]\n",
    "            if any(j < 0 for j in new_pos):\n",
    "                legal_actions.pop()\n",
    "        except IndexError:\n",
    "            legal_actions.pop()\n",
    "            \n",
    "        \n",
    "    return legal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def take_action(pos, action):\n",
    "    # Take action in the actual environment and return a reward and new position based on environment dynamics\n",
    "    if numpy.random.rand(1) < trans_prob:\n",
    "        new_pos = numpy.add(pos, action)\n",
    "    else:\n",
    "        new_pos = numpy.add(pos, random.choice(get_legal_actions(pos)))\n",
    "    reward = environment[new_pos[0]][new_pos[1]]\n",
    "    return new_pos, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mb_internal_environment_value(pos, mb_internal_environment):\n",
    "    value = mb_internal_environment[pos[0], pos[1]]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_mb_internal_environment(pos, next_pos, reward, mb_internal_environment):\n",
    "    next_value_list = list()\n",
    "    for i in get_legal_actions(next_pos):\n",
    "        next_value_list.append(mb_internal_environment[next_pos[0]][next_pos[1]])\n",
    "    next_value = max(next_value_list)\n",
    "    mb_internal_environment[pos[0], pos[1]] = (1-alpha) * mb_internal_environment_value(pos, mb_internal_environment) + (alpha) * (reward + gamma*next_value)\n",
    "    return mb_internal_environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dls(pos, curr_depth, max_depth, mb_internal_environment):\n",
    "    value_list = list()\n",
    "    if curr_depth == max_depth:\n",
    "        return mb_internal_environment_value(pos, mb_internal_environment)\n",
    "    else:\n",
    "        actions = get_legal_actions(pos)\n",
    "        for i in actions:\n",
    "            next_pos = numpy.add(numpy.array(pos), numpy.array(i))\n",
    "            value = mb_internal_environment_value(pos, mb_internal_environment) + gamma * dls(next_pos, curr_depth+1, max_depth, mb_internal_environment)\n",
    "            value_list.append(value)\n",
    "        if value_list:\n",
    "            average_value = numpy.mean(numpy.array(value_list))\n",
    "    return average_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dls_action(pos, max_depth, mb_internal_environment):\n",
    "    # Do depth Limited Search till max_depth on the mb_internal_environment\n",
    "    actions = get_legal_actions(pos)\n",
    "    next_value_list = list()\n",
    "    next_pos_list = list()\n",
    "    for i in actions:\n",
    "        next_pos = numpy.add(numpy.array(pos), numpy.array(i))\n",
    "        next_value_list.append(dls(next_pos, 0, max_depth, mb_internal_environment))\n",
    "        next_pos_list.append(next_pos)\n",
    "    zipped = list(zip(actions, next_value_list))\n",
    "    random.shuffle(zipped)\n",
    "    actions, next_value_list = zip(*zipped)\n",
    "    action = actions[numpy.argmax(next_value_list)]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_action(pos, mb_internal_environment):\n",
    "    # Look for q values in mf_internal_environment for given position\n",
    "    actions = get_legal_actions(pos)\n",
    "    next_value_list = list()\n",
    "    next_pos_list = list()\n",
    "    for i in actions:\n",
    "        next_pos = numpy.add(numpy.array(pos), numpy.array(i))\n",
    "        next_value_list.append(mb_internal_environment[next_pos[0], next_pos[1]])\n",
    "        next_pos_list.append(next_pos)\n",
    "    zipped = list(zip(actions, next_value_list))\n",
    "    random.shuffle(zipped)\n",
    "    actions, next_value_list = zip(*zipped)\n",
    "    action = actions[numpy.argmax(next_value_list)]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dual(num_iters, mb_internal_environment, change_env, change_at):\n",
    "    sum_reward = 0\n",
    "    max_depth = 3\n",
    "    steps_per_reward = 0\n",
    "    steps = list()\n",
    "    time_per_reward = list()\n",
    "    first_action = list()\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        pos = [3, 3]\n",
    "        j = 0\n",
    "        goal_start_time = time.time()\n",
    "        #change the environment at moderate training\n",
    "        if change_env and i == change_at:\n",
    "            environment[6, 6] = -10\n",
    "            environment[0, 0] = 10\n",
    "        while True:\n",
    "            if j%(i+1) == 0:\n",
    "                action = dls_action(pos, max_depth, mb_internal_environment)\n",
    "                next_pos, reward = take_action(pos, action)\n",
    "                mb_internal_environment = update_mb_internal_environment(pos, next_pos, reward, mb_internal_environment)\n",
    "                pos = next_pos\n",
    "            else:\n",
    "                action = q_action(pos, mb_internal_environment)\n",
    "                next_pos, reward = take_action(pos, action)\n",
    "                mb_internal_environment = update_mb_internal_environment(pos, next_pos, reward, mb_internal_environment)\n",
    "                pos = next_pos\n",
    "            sum_reward = 0.5*sum_reward + 0.5*reward\n",
    "            steps_per_reward += 1\n",
    "            if j == 0:\n",
    "                first_action.append(action)\n",
    "            j += 1\n",
    "            if reward != 0:\n",
    "                mb_internal_environment[pos[0], pos[1]] = (1-alpha) * mb_internal_environment_value(pos, mb_internal_environment) + (alpha) * (reward)\n",
    "                steps.append(steps_per_reward)\n",
    "                steps_per_reward = 0\n",
    "                #print \"here\", pos\n",
    "                goal_time = time.time()\n",
    "                time_per_reward.append(goal_time - goal_start_time)\n",
    "                \n",
    "                break\n",
    "    return sum_reward, steps, mb_internal_environment, time_per_reward, first_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mb(num_iters, mb_internal_environment):\n",
    "    sum_reward = 0\n",
    "    max_depth = 3\n",
    "    steps = list()\n",
    "    time_per_reward = list()\n",
    "    for i in range(num_iters):\n",
    "        pos = [0, 0]\n",
    "        steps_per_reward = 0\n",
    "        goal_start_time = time.time()\n",
    "        while True:\n",
    "            action = dls_action(pos, max_depth, mb_internal_environment)\n",
    "            next_pos, reward = take_action(pos, action)\n",
    "            mb_internal_environemnt = update_mb_internal_environment(pos, next_pos, reward, mb_internal_environment)\n",
    "            pos = next_pos\n",
    "            sum_reward = 0.5*sum_reward + 0.5*reward\n",
    "            steps_per_reward += 1\n",
    "            if reward != 0:\n",
    "                mb_internal_environment[pos[0], pos[1]] = (1-alpha) * mb_internal_environment_value(pos, mb_internal_environment) + (alpha) * (reward)                \n",
    "                steps.append(steps_per_reward)\n",
    "                steps_per_reward = 0\n",
    "                goal_time = time.time()\n",
    "                time_per_reward.append(goal_time - goal_start_time)\n",
    "                #print \"here\"\n",
    "                break\n",
    "\n",
    "    return sum_reward, steps, mb_internal_environment, time_per_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mf(num_iters, mb_internal_environment):\n",
    "    sum_reward = 0\n",
    "    steps = list()\n",
    "    time_per_reward = list()\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        pos = [0, 0]\n",
    "        steps_per_reward = 0        \n",
    "        #print (mb_internal_environment)\n",
    "        goal_start_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            action = q_action(pos, mb_internal_environment)\n",
    "            next_pos, reward = take_action(pos, action)\n",
    "            mb_internal_environment = update_mb_internal_environment(pos, next_pos, reward, mb_internal_environment)\n",
    "            pos = next_pos\n",
    "            sum_reward = 0.5*sum_reward + 0.5*reward\n",
    "            steps_per_reward += 1\n",
    "            if reward != 0:\n",
    "                \n",
    "                mb_internal_environment[pos[0], pos[1]] = (1-alpha) * mb_internal_environment_value(pos, mb_internal_environment) + (alpha) * (reward)\n",
    "                steps.append(steps_per_reward)\n",
    "                steps_per_reward = 0\n",
    "                goal_time = time.time()\n",
    "                time_per_reward.append(goal_time - goal_start_time)\n",
    "                \n",
    "                break\n",
    "            \n",
    "    return sum_reward, steps, mb_internal_environment, mb_internal_environment, time_per_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Dual Process on grid world. Get a list of individual time taken for each and rewards obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outcome devaluation for overtraining. (Change = True, change_at = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_dual = list()\n",
    "reward_dual = list()\n",
    "steps_dual = list()\n",
    "time_per_reward_dual = list()\n",
    "first_action_dual = list()\n",
    "change_at = 30\n",
    "iterations = 50\n",
    "change = True\n",
    "trials = 10\n",
    "for i in range(trials):\n",
    "    environment = numpy.zeros((7, 7))\n",
    "    trans_prob = 0.7\n",
    "    environment[6, 6] = 10\n",
    "    #environment[8, 8] = -10\n",
    "    environment[0, 0] = -10\n",
    "    mb_internal_environment = numpy.zeros((7, 7))\n",
    "    #mb_internal_environment[6, 6] = 10\n",
    "    start_time = time.time()\n",
    "    dual_result = dual(iterations, mb_internal_environment, change, change_at)\n",
    "    reward_dual.append(dual_result[0])\n",
    "    steps_dual.append(dual_result[1])\n",
    "    times_dual.append(time.time() - start_time)\n",
    "    time_per_reward_dual.append(dual_result[3])\n",
    "    first_action_dual.append(dual_result[4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 8]\n",
      "0.745\n"
     ]
    }
   ],
   "source": [
    "#print collections.Counter(first_action_dual)\n",
    "#print first_action_dual\n",
    "stay_percentage_overtrain = list()\n",
    "print collections.Counter(tuple(i) for i in first_action_dual[0][change_at:]).values()\n",
    "for i in range(trials):\n",
    "    stay_percentage_overtrain.append(max(collections.Counter(tuple(i) for i in first_action_dual[i][change_at:]).values())/float(iterations-change_at))\n",
    "print numpy.mean(stay_percentage_overtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outcome Devaluation for moderately trained. (Change == True, change_at = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times_dual = list()\n",
    "reward_dual = list()\n",
    "steps_dual = list()\n",
    "time_per_reward_dual = list()\n",
    "first_action_dual = list()\n",
    "change_at = 10\n",
    "iterations = 50\n",
    "change = True\n",
    "trials = 10\n",
    "for i in range(trials):\n",
    "    environment = numpy.zeros((7, 7))\n",
    "    trans_prob = 0.7\n",
    "    environment[6, 6] = 10\n",
    "    #environment[8, 8] = -10\n",
    "    environment[0, 0] = -10\n",
    "    mb_internal_environment = numpy.zeros((7, 7))\n",
    "    #mb_internal_environment[6, 6] = 10\n",
    "    start_time = time.time()\n",
    "    dual_result = dual(iterations, mb_internal_environment, change, change_at)\n",
    "    reward_dual.append(dual_result[0])\n",
    "    steps_dual.append(dual_result[1])\n",
    "    times_dual.append(time.time() - start_time)\n",
    "    time_per_reward_dual.append(dual_result[3])\n",
    "    first_action_dual.append(dual_result[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 29]\n",
      "0.275\n"
     ]
    }
   ],
   "source": [
    "#print collections.Counter(first_action_dual)\n",
    "stay_percentage_moderatelytrained = list()\n",
    "print collections.Counter(tuple(i) for i in first_action_dual[0][change_at:]).values()\n",
    "for i in range(trials):\n",
    "    stay_percentage_moderatelytrained.append((1 - max(collections.Counter(tuple(i) for i in first_action_dual[i][change_at:]).values())/float(iterations-change_at)))\n",
    "print numpy.mean(stay_percentage_moderatelytrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc5649ecf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stay_percentage_labels = [\"Overtrained\", \"Moderately trained\"]\n",
    "plt.bar(numpy.arange(2), (numpy.mean(stay_percentage_overtrain), numpy.mean(stay_percentage_moderatelytrained)), color = 'b')\n",
    "plt.xticks(numpy.arange(2), stay_percentage_labels)\n",
    "plt.ylabel(\"First Action Stay Percentage\")\n",
    "plt.title(\"Outcome Devaluation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times_mb = list()\n",
    "reward_mb = list()\n",
    "steps_mb = list()\n",
    "time_per_reward_mb = list()\n",
    "for i in range(10):\n",
    "    mb_internal_environment = numpy.zeros((7, 7))\n",
    "    #mb_internal_environment[6, 6] = 10\n",
    "    start_time = time.time()\n",
    "    mb_result = mb(30, mb_internal_environment)\n",
    "    reward_mb.append(mb_result[0])\n",
    "    steps_mb.append(mb_result[1])\n",
    "    times_mb.append(time.time() - start_time)\n",
    "    time_per_reward_mb.append(mb_result[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times_mf = list()\n",
    "reward_mf = list()\n",
    "steps_mf = list()\n",
    "time_per_reward_mf = list()\n",
    "for i in range(10):\n",
    "    mb_internal_environment = numpy.zeros((7, 7))\n",
    "    #mb_internal_environment[6, 6] = 10\n",
    "    start_time = time.time()\n",
    "    mf_result = mf(30, mb_internal_environment)\n",
    "    reward_mf.append(mf_result[0])\n",
    "    steps_mf.append(mf_result[1])\n",
    "    times_mf.append(time.time() - start_time)\n",
    "    time_per_reward_mf.append(mf_result[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Plotting time to complete one trial -- one walk through the environment. The walk completes on reward. \n",
    "The plot shows the dual process approaches TD in completing the walk through the environment -- showing goal directed behavior converted to habits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timeperrewardmb_plot = plt.plot(numpy.arange(0, 30), numpy.mean(numpy.array(time_per_reward_mb), axis=0), color = 'b', label = 'MB')\n",
    "timeperrewardmf_plot = plt.plot(numpy.arange(0, 30), numpy.mean(numpy.array(time_per_reward_mf), axis=0), color = 'r', label = 'MF')\n",
    "timeperrewarddual_plot = plt.plot(numpy.arange(0, 30), numpy.mean(numpy.array(time_per_reward_dual), axis=0), color = 'g', label = 'Dual')\n",
    "plt.legend()\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Time to reach goal')\n",
    "\n",
    "#plt.xticks(numpy.arange(3), ('mb', 'dual', 'mf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_mean_dual = numpy.mean(numpy.array(reward_dual))\n",
    "reward_mean_mb = numpy.mean(numpy.array(reward_mb))\n",
    "reward_mean_mf = numpy.mean(numpy.array(reward_mf))\n",
    "times_mean_dual = numpy.mean(numpy.array(times_dual))\n",
    "times_mean_mb = numpy.mean(numpy.array(times_mb))\n",
    "times_mean_mf = numpy.mean(numpy.array(times_mf))\n",
    "steps_mean_dual = numpy.mean(steps_dual)\n",
    "steps_mean_mb = numpy.mean(steps_mb)\n",
    "steps_mean_mf = numpy.mean(steps_mf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print steps_mean_dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stepsmf_plot, = plt.plot(numpy.arange(0, 30), numpy.mean(numpy.array(steps_mf), axis=0), color = 'r', label = 'MF')\n",
    "stepsdual_plot, = plt.plot(numpy.arange(0, 30), numpy.mean(numpy.array(steps_dual), axis=0), color = 'g', label = 'Dual')\n",
    "stepsmb_plot, = plt.plot(numpy.arange(0, 30), numpy.mean(numpy.array(steps_mb), axis=0), color = 'b', label = 'MB')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rewards = numpy.array([reward_mean_mb, reward_mean_dual, reward_mean_mf])\n",
    "#yerror = numpy.array(reward_errors)\n",
    "print (rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_plot, = plt.plot(numpy.arange(0, 3), rewards)\n",
    "plt.xticks(numpy.arange(3), ('mb', 'dual', 'mf'))\n",
    "plt.xlim(xmin = -1, xmax = 4)\n",
    "#plt.ylim(ymin = 4.5, ymax = 5.5)\n",
    "plt.xlabel('RL methods')\n",
    "plt.ylabel(\"Rewards\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times = numpy.array([times_mean_mb, times_mean_dual, times_mean_mf])\n",
    "print (times)\n",
    "plt.bar(numpy.arange(3), times, align='center')\n",
    "plt.xticks(numpy.arange(3), ('mb', 'dual', 'mf'))\n",
    "plt.xlim(xmin = -1, xmax = 4)\n",
    "#plt.ylim(ymin = -1, ymax = 10)\n",
    "plt.xlabel('RL methods')\n",
    "plt.ylabel(\"Time (in secs)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps = numpy.array([steps_mean_mb, steps_mean_dual, steps_mean_mf])\n",
    "print (steps)\n",
    "plt.bar(numpy.arange(3), steps, align='center')\n",
    "plt.xticks(numpy.arange(3), ('mb', 'dual', 'mf'))\n",
    "plt.xlim(xmin = -1, xmax = 3)\n",
    "#plt.ylim(ymin = 350, ymax = 400)\n",
    "plt.xlabel('RL methods')\n",
    "plt.ylabel(\"Stpes\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
